{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -y\n",
    "!python3 -m pip install pandas\n",
    "!python3 -m pip install plotly\n",
    "!python3 -m pip install numpy==1.26.4\n",
    "!python3 -m pip install matplotlib\n",
    "!python3 -m pip install ipywidgets==8.1.2\n",
    "!python3 -m pip install scipy\n",
    "!python3 -m pip install umap umap-learn\n",
    "!python3 -m pip install tqdm\n",
    "!python3 -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sys import getsizeof\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from math import*\n",
    "import plotly.express as px\n",
    "from scipy.special import logsumexp\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "from plotly.colors import n_colors\n",
    "from ipywidgets import interact,Layout,GridspecLayout, Button\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import plotly.offline as py\n",
    "from matplotlib import pyplot as plt\n",
    "import radviz_2d\n",
    "py.init_notebook_mode(connected=True)\n",
    "#import plotly.io as pio\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af621785-fb7d-47bc-a2e0-5727b531dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check widgets are working\n",
    "widgets.Button(\n",
    "    description='Run',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_user_selection(a):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            start = datetime.now()\n",
    "            select_country = dropbox1.value\n",
    "            country_list= [str(x).split('.')[0] for x in dropbox1.value]\n",
    "            country_name = [str(x).split('(')[0] for x in country_list]\n",
    "            country= ','.join(sorted(country_name))\n",
    "            print(\"country/countries selected: \",country)\n",
    "            sample_size = dropbox3.value\n",
    "            timestamp = dropbox4.value\n",
    "            region = dropbox2.value\n",
    "            lineage = dropbox6.value\n",
    "            country_timestamp_region = [country,timestamp,region,lineage]\n",
    "            c_t_r = ','.join(country_timestamp_region)\n",
    "            country_len = len(select_country)\n",
    "            sample_len = sample_size//country_len\n",
    "\n",
    "            col_name =  ['Week','Host','Date','Country','Pangolin Clade','EPI_ID','x','y']\n",
    "            timestamp_dict = {'oct-april_2023':'Last_6months','april2022-april2023':'Last_12months',\n",
    "                              'april2021-april2024':'Last_24months','jan-jun_2020':'jan-jun_2020',\n",
    "                              'jul-dec_2020':'jul-dec_2020', 'jan-dec_2020':'jan-dec_2020', 'jan-dec_2021':'jan-dec_2021','jan-jun_2021':'jan-jun_2021','jul-dec_2021':'jul-dec_2021',\n",
    "                              'jan-jun_2022':'jan-jun_2022', 'jan-dec_2022':'jan-dec_2022', 'jul-dec_2022':'jul-dec_2022','jan-jun_2023':'jan_jun_2023','whole':'whole'}\n",
    "            \n",
    "            get_time_val = timestamp_dict[timestamp]\n",
    "            print(get_time_val)\n",
    "            def pick_timeline(t):\n",
    "                if t == 'Last_6months':\n",
    "                    idx=(meta[:,2]> '2022-10-17') & (meta[:,2]<='2023-04-17')\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "                   \n",
    "                elif t == 'Last_12months':\n",
    "                    idx=(meta[:,2]> '2022-04-17') & (meta[:,2]<='2023-04-17')\n",
    "                    result=  np.where(idx)[0]\n",
    "                    return result\n",
    "                   \n",
    "                elif t == 'Last_24months':\n",
    "                    idx=(meta[:,2]> '2021-04-17') & (meta[:,2]<='2023-04-17')\n",
    "                    result = np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jan-jun_2020':\n",
    "                    idx=(meta[:,2]> '2020-01-01') & (meta[:,2]<='2020-06-30')#2020-06-30\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jan-dec_2020':\n",
    "                    idx=(meta[:,2]> '2020-01-01') & (meta[:,2]<='2020-12-31') #2021-06-30\n",
    "                    result = np.where(idx)[0]\n",
    "                    return result\n",
    "                    \n",
    "                elif t == 'jul-dec_2020':\n",
    "                    print(\"hello\")\n",
    "                    idx=(meta[:,2]> '2020-07-01') & (meta[:,2]<='2020-12-31')\n",
    "                    result=  np.where(idx)[0]\n",
    "                    return result\n",
    "                   \n",
    "                elif t == 'jan-jun_2021':\n",
    "                    idx=(meta[:,2]> '2021-01-01') & (meta[:,2]<='2021-06-30') #2021-06-30\n",
    "                    result = np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jan-dec_2021':\n",
    "                    idx=(meta[:,2]> '2021-01-01') & (meta[:,2]<='2021-12-31') #2021-06-30\n",
    "                    result = np.where(idx)[0]\n",
    "                    return result\n",
    "                  \n",
    "                elif t == 'jul-dec_2021':\n",
    "                    idx=(meta[:,2]> '2021-07-01') & (meta[:,2]<='2021-12-31')\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "            \n",
    "                elif t == 'jan-jun_2022':\n",
    "                    idx=(meta[:,2]> '2022-01-01') & (meta[:,2]<='2022-12-31') # 2022-06-30\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jul-dec_2022':\n",
    "                    idx=(meta[:,2]> '2022-07-01') & (meta[:,2]<='2022-12-31')\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jan-dec_2022':\n",
    "                    idx=(meta[:,2]> '2022-01-01') & (meta[:,2]<='2022-12-31') #2021-06-30\n",
    "                    result = np.where(idx)[0]\n",
    "                    return result\n",
    "\n",
    "                elif t == 'jan-jun_2023':\n",
    "                    idx=(meta[:,2]> '2023-01-01') & (meta[:,2]<='2023-06-30')\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "                    \n",
    "                elif t == 'whole':\n",
    "                    idx=(meta[:,2]>= '2020-01-01') & (meta[:,2]<='2022-06-30')\n",
    "                    result= np.where(idx)[0]\n",
    "                    return result\n",
    "                   \n",
    "                \n",
    "            ######### reading the file/files, perform random sampling based on samplesize and putting them into dataframe######################3\n",
    "            checkpoint =[]\n",
    "            metadata = []\n",
    "            for filename in tqdm(select_country,desc= \"loading file\"):\n",
    "                new_list = []\n",
    "                with open(\"meta_new/%s.tsv\" % filename.split(\"(\")[0]) as fin:\n",
    "                    for line in fin:\n",
    "                        line = line.rstrip().split(\"\\t\")\n",
    "                        new_list.append(line)\n",
    "                meta= np.array(new_list)\n",
    "                #print(meta.shape)\n",
    "                idx_meta = pick_timeline(get_time_val)\n",
    "                if idx_meta.size == 0:\n",
    "                    print(\"\\033[1;31m No datapoints found for specified criteria  \\n\")\n",
    "                    return\n",
    "                \n",
    "\n",
    "                index_meta = list(idx_meta)\n",
    "                #print(len(index_meta))\n",
    "                np.random.seed(42)\n",
    "                sample_index_meta = np.random.choice(index_meta,min(len(index_meta),sample_len),replace =False)\n",
    "                #print(sample_index_meta)\n",
    "                with open(\"data_npz_new/%s.npz\" % filename.split(\"(\")[0], 'rb') as f:\n",
    "                    data_matrix = sparse.load_npz(f)                    \n",
    "                    #np.random.seed(42)\n",
    "                    m = data_matrix[sample_index_meta]\n",
    "                    X_m = m.toarray()\n",
    "                    z_m = meta[sample_index_meta, :]\n",
    "                    checkpoint.append(X_m)\n",
    "                    metadata.append(z_m)\n",
    "            X =np.concatenate(checkpoint)\n",
    "            z = np.concatenate(metadata)\n",
    "            #print(z)\n",
    "            \n",
    "            \n",
    "           \n",
    "            if dropbox6.value == 'All':\n",
    "                \n",
    "                X =np.concatenate(checkpoint)\n",
    "                z = np.concatenate(metadata)\n",
    "\n",
    "            elif dropbox6.value == 'Alpha':\n",
    "                \n",
    "                index_meta = np.where((z == 'B.1.1.7'))[0]\n",
    "                #print(index_meta)\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis)\n",
    "                \n",
    "            elif dropbox6.value == 'Beta':\n",
    "                \n",
    "                index_meta = np.where((z == 'B.1.351'))[0]\n",
    "                #print(index_meta)\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis)\n",
    "            elif dropbox6.value == 'Gamma':\n",
    "                \n",
    "                index_meta = np.where((z == 'P.1'))[0]\n",
    "                #print(index_meta)\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis)\n",
    "            elif dropbox6.value == 'Delta':\n",
    "                \n",
    "                index_meta = np.where((z == 'B.1.617')|(z == 'B.1.617.1')|(z=='B.1.617.2')|(z=='B.1.617.3'))[0]\n",
    "                #print(index_meta)\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis)\n",
    "                #print(len(z))\n",
    "            elif dropbox6.value == 'Omicron':\n",
    "                \n",
    "                #index_meta = np.where((z == 'BA.1')|(z== 'BA.1.1')|(z== 'BA.2')|(z== 'BA.3')|(z== 'BA.4')|(z== 'BA.5'))[0]\n",
    "                index_meta = np.where((z== 'B.1.1.529')| (z == 'BA.1')|(z== 'BA.1.1')|(z== 'BA.2')|(z== 'BA.3')|(z== 'BA.4')|(z== 'BA.5'))[0] #In BOV Check once\n",
    "                #print(index_meta)\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis)  \n",
    "            elif dropbox6.value == 'VOC_All':\n",
    "                \n",
    "                index_meta = np.where((z == 'BA.1')|(z== 'B.1.1.7')|(z== 'BA.2')|(z== 'BA.3')|(z== 'BA.4')|(z== 'BA.5')|(z== 'B.1.617.2')|(z== 'P.1')|(z== 'B.1.351'))[0]\n",
    "                #print(index_meta)\n",
    "                #\n",
    "                axis = 0\n",
    "                z = np.take(z, index_meta, axis)\n",
    "                X= np.take(X, index_meta,axis) \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            #print(\"Total memory used by loaded data: \",getsizeof(X))\n",
    "            #################################################################################################\n",
    "\n",
    "            ######## function to pick functional genomic region of interest#####################################\n",
    "            def pick_region(v):\n",
    "                if v == 'whole':\n",
    "                    whole = X\n",
    "                    return whole\n",
    "                elif v == 'spike':\n",
    "                    spike = np.array(X[:,55034:65385]).astype(np.int8)\n",
    "                    return spike\n",
    "                elif v == 'membrane':\n",
    "                    membrane = np.array(X[:,68597:70341]).astype(np.int8)\n",
    "                    return membrane\n",
    "                elif v == 'envelope':\n",
    "                    envelope = np.array(X[:,67849:68450]).astype(np.int8)\n",
    "                    return envelope\n",
    "                elif v == 'nucleocapsid':\n",
    "                    nucleocapsid = np.array(X[:,73564:77025]).astype(np.int8)\n",
    "                    return nucleocapsid\n",
    "                return val\n",
    "                \n",
    "            \n",
    "            val = pick_region(dropbox2.value)\n",
    "            print(\"resulting vector after filtering\",val.shape)\n",
    "\n",
    "           \n",
    "            print(\"Final dimension of feature vectors:\", val.shape)\n",
    "            if val.shape[0] == 0:\n",
    "                print(\"\\033[1;31m No datapoints found for specified criteria  \\n\")\n",
    "                return\n",
    "            #np.savetxt(\"foo.csv\", val, fmt=\"%d\", delimiter=\",\")\n",
    "            \n",
    "            flatten_arr = np.ravel(val)\n",
    "            result = np.all(val==flatten_arr[0])\n",
    "            if result :\n",
    "                print(\"\\033[1;31m There are no variation found in the specified selection, all datapoints are similar  \\n\")\n",
    "                return\n",
    "            '''def unique_rows(a):\n",
    "                a = np.ascontiguousarray(a)\n",
    "                unique_a = np.unique(a.view([('', a.dtype)]*a.shape[1]))\n",
    "                return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))'''\n",
    "            '''def unique_rows(a):\n",
    "                ind = np.lexsort(a.T)\n",
    "                return a[np.concatenate(([True],np.any(a[ind[1:]]!= a[ind[:-1]],axis=1)))]\n",
    "    \n",
    "\n",
    "            \n",
    "            uniq = unique_rows(val)\n",
    "            print(len(uniq))'''\n",
    "            \n",
    "              \n",
    "            #######################################################################################################\n",
    "            \n",
    "            ########### Perform chunking, normalization, dimensionality reduction on data########################\n",
    "            RP_sparse= SparseRandomProjection(n_components=20,random_state=42)\n",
    "            pca = PCA(n_components=2,random_state=42)\n",
    "            def normalise(X_mod):\n",
    "                row_sums=[]\n",
    "                for i in range(0,X_mod.shape[0]):\n",
    "                    rowsumval=np.sum(X_mod[i]**2)\n",
    "                    row_sums.append(math.sqrt(rowsumval))\n",
    "\n",
    "                rowsums=np.asarray(row_sums)\n",
    "                np.seterr(divide='ignore', invalid='ignore')\n",
    "                A_norm = np.divide(X_mod,rowsums[:,np.newaxis]).astype(np.float32) #row normalized input \n",
    "                A_norm[np.isnan(A_norm)] = 0\n",
    "                return A_norm\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            chunk_size = 2000\n",
    "            total_size = val.shape[0]\n",
    "            min_size = (min(total_size,sample_size))\n",
    "            if min_size > 2000:\n",
    "                if not min_size % 2000 == 0:\n",
    "                    get_val = (min_size//chunk_size)+1\n",
    "                else:\n",
    "                    get_val = sample_size//chunk_size\n",
    "                \n",
    "            else:\n",
    "                get_val = 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            matrix=[]\n",
    "            for i in tqdm(range(0, get_val),desc=\"Performing dimensionality reduction on feature vectors\"):\n",
    "                X_val = val[i*chunk_size : (i+1)*chunk_size]\n",
    "                X_normalized = normalise(X_val)\n",
    "                X_embed_partial= RP_sparse.fit_transform(X_normalized)\n",
    "                del X_normalized\n",
    "                gc.collect()\n",
    "                matrix.append(X_embed_partial)\n",
    "            X_embed = np.concatenate(matrix)\n",
    "\n",
    "\n",
    "            #RP_sparse= SparseRandomProjection(n_components=15)\n",
    "            #X_normalized = normalise(val)\n",
    "            #X_embed = pca.fit_transform(X_normalized)\n",
    "            #X_embed = RP_sparse.fit_transform(X_normalized)\n",
    "            print(\"Dimension after dimensionality reduction on feature vectors: \", X_embed.shape)\n",
    " \n",
    "            ##############################################################################################################################################################\n",
    "\n",
    "            ######### Appropriate k val calculation if country is not present in dictionary#########################################################\n",
    "           \n",
    "            def k_val_max(val):\n",
    "                no_of_clusters = [i for i in range(2,15)] \n",
    "                maxim=-1\n",
    "                max_clust=-1\n",
    "                scores=[]\n",
    "\n",
    "                for n_cluster in tqdm(no_of_clusters,desc=\"Calculating appropriate k value\"):\n",
    "                    try:\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "                            cluster = KMeans(n_clusters = n_cluster)\n",
    "                            cluster_labels = cluster.fit_predict(sample_kclust)\n",
    "                            #print(cluster_labels)\n",
    "                            silhouette_avg = silhouette_score(sample_kclust, cluster_labels)\n",
    "                            scores.append(silhouette_avg)\n",
    "                            if (silhouette_avg > maxim):\n",
    "                                maxim=silhouette_avg\n",
    "                                max_clust=n_cluster\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    \n",
    "                return max_clust\n",
    "\n",
    "        \n",
    "                    \n",
    "\n",
    "            sample_kclust = X_embed[np.random.choice(X_embed.shape[0],min(X_embed.shape[0],5000), replace=False), :]\n",
    "            #print(\"dimension after random sampling for k value calculation: \",sample_kclust.shape)\n",
    "            filename = \"kval_lookup.txt\"\n",
    "            filesize = os.path.getsize(filename)\n",
    "\n",
    "            if filesize == 0:\n",
    "                max_clust = k_val_max(sample_kclust)\n",
    "                country_timestamp_region.append(max_clust)\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(\"\\t\".join(str(item) for item in country_timestamp_region))\n",
    "\n",
    "            else:\n",
    "                dictionary={}\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        key, value = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                        dictionary[key] = value\n",
    "\n",
    "                    key = c_t_r\n",
    "                    if key not in dictionary:\n",
    "                        max_clust = k_val_max(sample_kclust)\n",
    "                        if max_clust == -1:\n",
    "                            max_clust\n",
    "                        else:\n",
    "                        \n",
    "                            dictionary.update({key:max_clust})\n",
    "                        #print(\"Dictionary: \",dictionary)\n",
    "                        \n",
    "                        with open(filename, 'w') as f:\n",
    "                            for key, value in dictionary.items(): \n",
    "                                f.write('%s\\t%s\\n' % (key, value)) \n",
    "\n",
    "                    else:\n",
    "                        print(\" Optimal K value for selected criteria is: \",int(dictionary[key]))\n",
    "                        max_clust = int(dictionary[key])\n",
    "    \n",
    "\n",
    "                   \n",
    "            ####################################################################################################################\n",
    "            ################# K-means clustering ##########################################################################\n",
    "            if max_clust == -1:\n",
    "                print(\"\\033[1;31m  Selected samples had less variation in it, Clustering appointed all datapoints to similar cluster\")\n",
    "                return\n",
    "                \n",
    "            kmeans = KMeans(n_clusters = max_clust,random_state=42)\n",
    "            kmeans.fit(X_embed)\n",
    "            centroids  = kmeans.cluster_centers_\n",
    "            labels = kmeans.labels_\n",
    "            #print(labels)\n",
    "            \n",
    "            ##################################################################################################\n",
    "            \n",
    "            \n",
    "            ######################################################################################################\n",
    "            def ClusterIndicesNumpy(clustNum, labels_array): \n",
    "                #check = np.where(labels_array == clustNum)[0]\n",
    "                #print(\"check_indices\",len(check))\n",
    "                return np.where(labels_array == clustNum)[0]\n",
    "        \n",
    "            \n",
    "            def jaccard_similarity(argum):\n",
    "                #print(\"HELLO\")\n",
    "                x = argum[0]\n",
    "                y = argum[1]\n",
    "                centroid_sum = argum[2]\n",
    "                X_sum = argum[3]\n",
    "                numerator = np.dot(x,y)\n",
    "                #numerator = np.sum(np.minimum(x,y))\n",
    "                denominator = (centroid_sum-numerator)+X_sum\n",
    "                if denominator == 0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 1- (numerator/float(denominator))\n",
    "                #return 1- (numerator/float(denominator))\n",
    "            centroid_vector=[]\n",
    "            \n",
    "            centroid_sum=[]\n",
    "            for i in tqdm(range(0, max_clust),desc=\"calculating centroid  vectors \"):\n",
    "                data = val[ClusterIndicesNumpy(i,kmeans.labels_)]\n",
    "                centroid = data.mean(axis=0)\n",
    "                #print(\"centroid of the following is : \",centroid)\n",
    "                centroid_vector.append(centroid)\n",
    "                centroid_sum.append(np.sum(centroid))\n",
    "            #print(centroid_vector) \n",
    "            #np.savetxt(\"centroid.csv\", centroid_vector, delimiter=\",\")\n",
    "\n",
    "            def divide_chunks(l, n):\n",
    "                for i in range(0, len(l), n):\n",
    "                    yield l[i:i + n]\n",
    " \n",
    "            max_threads = 6\n",
    "            \n",
    "            all_val_arr =[]\n",
    "            jac_sim=[]\n",
    "            #jac_sim_all=[]\n",
    "            # Start processing the slides in parallel\n",
    "            for j in range(0, len(X_embed)):\n",
    "               \n",
    "                X_sum= np.sum(val[j])\n",
    "                \n",
    "                for k in range(0, len(centroid_vector)):\n",
    "                    all_val_arr.append((val[j],centroid_vector[k], centroid_sum[k],X_sum))\n",
    "            #print(len(all_val_arr))\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "                for p in tqdm(executor.map(jaccard_similarity, all_val_arr),desc=\"Calculating weighted jaccard similarity\"):\n",
    "                    if p==None: continue\n",
    "                    js = p\n",
    "                    #print(js)\n",
    "                    jac_sim.append(js)\n",
    "                \n",
    "                jac_sim_all = list(divide_chunks(jac_sim, len(centroid_vector)))\n",
    "            jaccard_metric = jac_sim_all\n",
    "            #print(jaccard_metric)\n",
    "            jc_met = np.array(jaccard_metric)\n",
    "            total_time_taken = datetime.now() - start\n",
    "            print(\"Total time taken : \", total_time_taken)\n",
    "            \n",
    "            def plot(type_plot):\n",
    "                if (type_plot == 'BOV_RadViz') | (type_plot == 'BOV_PCA'):\n",
    "                    ptype = \"Approach = BOVIZ\"\n",
    "                    \n",
    "\n",
    "                    def print_plot(grpval):\n",
    "                        pca = PCA(n_components=2)\n",
    "                        components = pca.fit_transform(jaccard_metric)\n",
    "                        #print(components)\n",
    "                        map2 ={}\n",
    "                        data = []\n",
    "\n",
    "                        for i in range(0,len(z)):\n",
    "                            map2[i] = z[i]\n",
    "                        for k,v in enumerate(components):\n",
    "                            meta = map2[k].tolist()\n",
    "                            comp = v.tolist()\n",
    "                            val = meta + comp\n",
    "                            data.append(val)\n",
    "\n",
    "                        df1 =pd.DataFrame(data,columns = col_name)\n",
    "                        country_name_com = list(df1['Country'].unique())\n",
    "                        plotname =  \",\".join([country_name_com[i] for i in range(len(country_name_com))])+\";\"+ptype\n",
    "                        #colors_data = pd.read_csv(\"Color_map.txt\",sep='\\t')\n",
    "                        #group_color = colors_data.set_index('Clade')['RGB'].to_dict()\n",
    "                        group_color = {'BA.4':'yellow','BA.2':'pink','BA.5':'violet','BA.3':'brown','B.1.351':'purple','P.1':'orange',\n",
    "                                       'BA.1':'green','B.1.617.2':'red','B.1.1.7':'blue','India':'green','South Africa':'blue','Brazil':'orange',\n",
    "                                      'United Kingdom': 'red'}\n",
    "                        df1['counts'] = df1['Pangolin Clade'].map(df1['Pangolin Clade'].value_counts())\n",
    "                        #print(df1['counts'])\n",
    "                        df1= df1.sort_values(by=['counts'], ascending=False)\n",
    "                        #print(df1)\n",
    "                        df1[\"Week\"] = df1[\"Week\"].astype(float) #convert back to numeric\n",
    "                        labels = {\n",
    "                            str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "                            for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "                                }\n",
    "                        #print(labels) \n",
    "                        #color_discrete_map=group_color\n",
    "                        new = {'BA.4':'BA.4','BA.2':'BA.2(Omicron)','BA.5':'BA.5','BA.3':'BA.3','B.1.351':'B.1.351(Beta)','P.1':'P.1(Gamma)',\n",
    "                                       'BA.1':'BA.1','B.1.617.2':'B.1.617.2(Delta)','B.1.1.7':'B.1.1.7(Alpha)'}\n",
    "                        fig = px.scatter(df1, x='x', y='y',color=grpval,color_discrete_map=group_color,hover_name='EPI_ID', \n",
    "                                         hover_data= {'x' : False,'y' : False,'Week': True,'Host': True,'Date': True,'Country': True,'Pangolin Clade':True}\n",
    "                                         ,labels=labels,width=1000, height=600)\n",
    "                        #fig.for_each_trace(lambda t: t.update(name = new[t.name]))\n",
    "                        fig.update_traces(marker=dict(size=10),\n",
    "                                                  selector=dict(mode='markers'))\n",
    "                        fig.update_layout(\n",
    "                                    title=plotname, #plot title \n",
    "                                    paper_bgcolor='white',\n",
    "                                    plot_bgcolor='white',\n",
    "                                    showlegend = True,\n",
    "                                    font=dict(\n",
    "                                        family=\"Courier New, monospace\",\n",
    "                                        size=18,\n",
    "                                        color=\"black\"\n",
    "                                    )\n",
    "                                )\n",
    "                        fig.update_xaxes(title_text=' ',\n",
    "                                       showgrid=True,\n",
    "                                       zeroline=True,\n",
    "                                       showline=True,\n",
    "                                       gridcolor='#bdbdbd',\n",
    "                                       gridwidth=2,\n",
    "                                       zerolinecolor='#969696',\n",
    "                                       zerolinewidth=4,\n",
    "                                       linecolor='#969696',\n",
    "                                       linewidth=4)\n",
    "                        fig.update_yaxes(title_text=' ',\n",
    "                                       showgrid=True,\n",
    "                                       zeroline=True,\n",
    "                                       showline=True,\n",
    "                                       gridcolor='#bdbdbd',\n",
    "                                       gridwidth=2,\n",
    "                                       zerolinecolor='#969696',\n",
    "                                       zerolinewidth=4,\n",
    "                                       linecolor='#969696',\n",
    "                                       linewidth=4)\n",
    "\n",
    "                        py.iplot(fig)\n",
    "                        #py.plot(fig, filename='check.html')\n",
    "                        \n",
    "                    im = interact(print_plot,  grpval= ['Country','Pangolin Clade','Week'])\n",
    "                    im.widget.children[0].description = 'Color code by : '\n",
    "                    im.widget.children[0].style = {'description_width': 'initial'}\n",
    "                    display(im)\n",
    "                \n",
    "                elif type_plot == 'radviz':\n",
    "                    def print_plot_radviz(grpval):\n",
    "                        ptype = \"Approach = BOV RadViz\"\n",
    "                        plotname =  \",\".join(str(x).split('(')[0] for x in country_list)+\";\"+ptype+\";\"+\"k=\"+str(max_clust)\n",
    "\n",
    "                        map_1 ={}\n",
    "                        data = []\n",
    "                        column_name =  ['Week','Host','Date','Country','Pangolin Clade','EPI_ID']\n",
    "                        field = list(range(0,max_clust))\n",
    "                        column_name.extend(field)\n",
    "                        for i in range(0,len(z)):\n",
    "                            map_1[i] = z[i]\n",
    "                        for k,v in enumerate(jc_met):\n",
    "                                meta = map_1[k].tolist()\n",
    "                                comp = v.tolist()\n",
    "                                val = meta + comp\n",
    "                                data.append(val)\n",
    "\n",
    "                        df1 =pd.DataFrame(data,columns = column_name)\n",
    "\n",
    "                        df1['counts'] = df1['Pangolin Clade'].map(df1['Pangolin Clade'].value_counts())\n",
    "                        df1= df1.sort_values(by=['counts'], ascending=False)\n",
    "                        df1[\"Week\"] = df1[\"Week\"].astype(float) #convert back to numeric\n",
    "\n",
    "                        y=df1[grpval]\n",
    "\n",
    "                        X=df1.iloc[:,6:-1]\n",
    "                        #print(X)\n",
    "                        BPs=1000\n",
    "\n",
    "                        radviz_2d.RadViz2D(y,X,BPs)\n",
    "                    im = interact(print_plot_radviz,  grpval= ['Country','Pangolin Clade','Week'])\n",
    "                    im.widget.children[0].description = 'Color code by : '\n",
    "                    im.widget.children[0].style = {'description_width': 'initial'}\n",
    "                    display(im)\n",
    "\n",
    "            print(\"max_clust \" ,max_clust)\n",
    "            if max_clust > 15:\n",
    "                if dropbox5.value == 'BOV_RadViz':\n",
    "                    print(\"\\033[1;32m The k value is more than specified range(k=10) for radviz, so we are running for PCA \\n\")\n",
    "                    plot(dropbox5.value)\n",
    "                elif dropbox5.value == 'BOV_PCA':\n",
    "                    print(\"\\033[1;32m we are running for PCA \\n\")\n",
    "                    plot(dropbox5.value)\n",
    "            \n",
    "            elif max_clust <=15 and dropbox5.value == 'BOV_RadViz':\n",
    "                print(\"\\033[1;32m The selected choice will proceed for visualization with radviz \\n\")\n",
    "                plot_name = 'radviz'\n",
    "                plot(plot_name)\n",
    "                \n",
    "                \n",
    "            elif max_clust <=15 and dropbox5.value == 'BOV_PCA':\n",
    "                print(\"\\033[1;32m The selected choice will proceed for visualization with PCA \\n\")\n",
    "                plot(dropbox5.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195667c4-3b3b-4559-9b20-bbe4f64c4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = pd.read_csv(\"country_data_analysis.txt\",sep='\\t')\n",
    "d = country_data.set_index('Country')['Samples'].to_dict()\n",
    "\n",
    "'''def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        print(\"Country selected: %s\" %change['new'])\n",
    "\n",
    "dropbox1.observe(on_change)'''\n",
    "\n",
    "out = widgets.Output()\n",
    "dropbox1 = widgets.SelectMultiple(options=['{}({})'.format(k,v) for k,v in d.items()],description='Country:', rows=8, disabled=False)\n",
    "\n",
    "#(Maximum datapoints allowed: 15000)\n",
    "dropbox2 = widgets.Dropdown(\n",
    "    options=['envelope','membrane','nucleocapsid','spike','whole'],\n",
    "    value='whole',\n",
    "    rows=5,\n",
    "    description='Genomic region of interest:',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dropbox3 = widgets.BoundedIntText(\n",
    "    value=10000,\n",
    "    max=15000,\n",
    "    step=500,\n",
    "    description='Datapoints on canvas:',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dropbox4 = widgets.Dropdown(\n",
    "    options=['oct-april_2023','april2022-april2023','april2021-april2024', 'jan-dec_2020','jan-jun_2020','jul-dec_2020','jan-dec_2021','jan-jun_2021','jul-dec_2021', 'jan-dec_2022', 'jan-jun_2022','jul-dec_2022','jan-jun_2023','whole'],\n",
    "    value='oct-april_2023',\n",
    "    rows=4,\n",
    "    #layout={'width': 'max-content'},\n",
    "    description='Timestamp:',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dropbox5 = widgets.Dropdown(\n",
    "    options=['BOV_PCA','BOV_RadViz'],\n",
    "    value='BOV_PCA',\n",
    "    rows=3,\n",
    "    #layout={'width': 'max-content'},\n",
    "    description='Type of plot',\n",
    "    style={'description_width': 'initial'},\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dropbox6 = widgets.Dropdown(\n",
    "    options=['Alpha','Beta','Gamma','Delta','Omicron','VOC_All','All'],\n",
    "    value= 'All',\n",
    "    rows=5,\n",
    "    description='Select Lineage:',\n",
    "    style={'description_width': 'initial'\n",
    "          },\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "def dropdown_eventhandler(change):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            country_list= [str(x).split('.')[0] for x in change.new]\n",
    "            country_name = [str(x).split('(')[0] for x in country_list]\n",
    "            country= ','.join(sorted(country_name))\n",
    "            print(\"\\033[1;32m Country/Countries selected: \", country)\n",
    "\n",
    "\n",
    "box_layout = widgets.Layout(display = 'flex',\n",
    "                flex_flow='column',\n",
    "                align_items='stretch',\n",
    "                width='60%',\n",
    "                height = '60%'\n",
    "                 )\n",
    "# Button to click\n",
    "select_button = widgets.Button(\n",
    "    description='Run',\n",
    "    disabled=False,\n",
    "    layout = box_layout,\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "dropbox1.observe(dropdown_eventhandler, names='value')\n",
    "\n",
    "select_button.on_click(get_user_selection)\n",
    "\n",
    "grid = GridspecLayout(5, 3, height='300px', align_items = 'baseline')\n",
    "grid[0, 1] = dropbox6\n",
    "grid[:, 0] = dropbox1\n",
    "grid[1, 1] = dropbox4\n",
    "grid[0, 2] = dropbox3\n",
    "grid[4, 1] = select_button\n",
    "grid[1,2] = dropbox5\n",
    "grid[2,1] = dropbox2\n",
    "\n",
    "\n",
    "'''ui = widgets.HBox([dropbox1,dropbox4,dropbox2])\n",
    "ui_1 = widgets.HBox([dropbox3,dropbox5])\n",
    "'''\n",
    "\n",
    "#ui_run = widgets.HBox(children=[select_button],layout=box_layout)\n",
    "display(grid,out)\n",
    "dropbox1.observe(dropdown_eventhandler, names='value')\n",
    "#display(ui,ui_1,ui_run,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd69723-4fd0-4b04-badd-74f27a37bc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-accessory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
